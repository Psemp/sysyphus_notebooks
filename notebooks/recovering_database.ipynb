{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import time\n",
    "import gc\n",
    "import pandas as pd\n",
    "import cloudscraper\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from matplotlib import pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from scripts import preprocessing\n",
    "\n",
    "load_dotenv()\n",
    "plt.style.use('Solarize_Light2')\n",
    "\n",
    "# Setting default DPI, pulling it from dotenv if it exists, setting it on 100 if not\n",
    "try:\n",
    "    pc_dpi = int(os.getenv('DPI'))\n",
    "except TypeError:\n",
    "    pc_dpi = 100\n",
    "if pc_dpi is None:\n",
    "    pc_dpi = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "The primary objectives of this project include:\n",
    "\n",
    "- **Feasibility of Database Recreation:** Evaluating the practicality of recreating a comprehensive meteorite database by aggregating data from a single, extensive request.\n",
    "- **Establishment of Initial Search Step:** Setting up the foundational step in the meteorite property search process, where one request retrieves all necessary URLs, types, names, places of fall, etc. This is aimed at reducing the overall impact on server resources compared to performing a new search request each time a user initiates a search. Instead, the user will search through a JSON file extracted from the webpage, not the page itself.\n",
    "- **Reproducibility:** Ensuring the process can be automatically updated with tools like GitHub Actions or other automation tools configured similarly to CRON jobs. The goal is for this notebook to pave the way for a script that facilitates the \"get-links\" pipeline, making the dataset self-updating and reliable over time.\n",
    "- **Handling Large Requests:** Recognizing that the request involves parsing over 4 million lines of HTML, which introduces potential for errors such as server timeouts or connection interruptions. Implementing checks is crucial, for example, verifying the end of the file for a closing body tag or closing HTML tag (`</html>`, `</body>`) to ensure the complete dataset is captured.\n",
    "- **Export Format:** The data should be exported as a JSON file with fields that are straightforward to navigate and search, making the dataset accessible and usable for various applications.\n",
    "\n",
    "This structured approach aims to minimize the load on the source server, streamline the data collection process, and ensure the sustainability and usability of the meteorite database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_zero = time.perf_counter()\n",
    "\n",
    "scraper = cloudscraper.create_scraper()\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/115.0\",\n",
    "}\n",
    "\n",
    "behemoth = \"\"\"https://www.lpi.usra.edu/meteor/metbull.php?sea=%2A&sfor=names&ants=\\\n",
    "&nwas=&falls=&valids=&stype=contains&lrec=100000&map=ge&browse=&country=\\\n",
    "All&srt=name&categ=All&mblist=All&rect=\\\n",
    "&phot=&strewn=&snew=0&pnt=Normal%20table&dr=&page=1\"\"\"\n",
    "\n",
    "r = scraper.get(url=behemoth, headers=headers, timeout=10)\n",
    "\n",
    "soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "\n",
    "t_end = time.perf_counter()\n",
    "\n",
    "print(f\"request time : {round(t_end - t_zero, 4)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's test that it has a closing body tag and/or closing html tag :\n",
    "\n",
    "(Let's use the tail of the html because its kindof a large file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_closing_tags_bytes(html_bytes: bytes) -> tuple:\n",
    "    \"\"\"\n",
    "    Check the last bytes of the HTML content for </body> and </html> tags.\n",
    "    This function is to be launched on the request.content bytes object.\n",
    "    \n",
    "    Args:\n",
    "        html_bytes (bytes): The HTML content as a bytes object.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing two booleans indicating the presence of </body> and </html> tags.\n",
    "    \"\"\"\n",
    "\n",
    "    tail_content = html_bytes[-100:].decode(\"utf-8\", errors=\"ignore\").lower()\n",
    "    \n",
    "    closing_body_tag = \"</body>\" in tail_content\n",
    "    closing_html_tag = \"</html>\" in tail_content\n",
    "\n",
    "    return closing_body_tag, closing_html_tag\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_tag, html_tag = check_closing_tags_bytes(r.content)\n",
    "\n",
    "print(f\"Closing body tag found : {body_tag}\")\n",
    "print(f\"Closing html tag found : {html_tag}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting the main table (we just need the main table)\n",
    "- We have confirmed correct EOF (end tags)\n",
    "- We just need the maintable id of the html doc\n",
    "- We'll free memory by removing the initial soup (we'll keep the r object just in case)\n",
    "- We'll force a gc.collect() to improve perfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we want the names, classes, and codes to get the URL\n",
    "main_table = soup.find(\"table\", id=\"maintable\")\n",
    "\n",
    "del soup\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now to extract the informations from the HTML :\n",
    "- We know :\n",
    "    - Each meteorite is registered as a span : `(\"span\", class_=\"mname\")` for soup\n",
    "    - Fields orders : Name|Abbrev|Status|Fall|Year|Place|Type|Mass|MetBull|GoogleEarth|Notes\n",
    "    - We need fields [0, 4, 5, 6, 7]\n",
    "    - We can deduce the url of the meteorite by its code, with it we can reform the url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteorites = main_table.find_all(\"span\", class_=\"mname\")\n",
    "print(f\"There are {meteorites.__len__()} meteorites in the html doc\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets form a dataframe out of this :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.lpi.usra.edu/meteor/metbull.php?\"\n",
    "meteorite_data = []\n",
    "\n",
    "for row in main_table.find_all('tr'):\n",
    "    cells = row.find_all('td')\n",
    "    if len(cells) > 8:  # Ensuring there are enough cells\n",
    "        anchor = cells[0].find(\"a\")\n",
    "        if anchor and \"href\" in anchor.attrs:\n",
    "            href = anchor[\"href\"]\n",
    "            code_start = href.find(\"code=\") + len(\"code=\")\n",
    "            met_code = href[code_start:].split(\"&\")[0] if \"&\" in href[code_start:] else href[code_start:]\n",
    "            met_url = f\"{base_url}code={met_code}\"\n",
    "        else:\n",
    "            met_url = None\n",
    "        \n",
    "        # Extracting the required fields\n",
    "        name = preprocessing.handle_name(cells[0].text.strip())\n",
    "        year = preprocessing.handle_year(cells[4].text.strip())\n",
    "        country = preprocessing.handle_country(cells[5].text.strip())\n",
    "        met_type = preprocessing.handle_types(cells[6].text.strip())\n",
    "        mass = preprocessing.handle_mass(cells[7].text.strip())\n",
    "\n",
    "        # Append this meteorite's info as a dict\n",
    "        meteorite_data.append({\n",
    "            \"name\": name,\n",
    "            \"year\": year,\n",
    "            \"country\": country,\n",
    "            \"type\": met_type,\n",
    "            \"mass\": mass,\n",
    "            \"URL\": met_url\n",
    "        })\n",
    "\n",
    "# Convert the list of dicts to a pandas DataFrame\n",
    "df = pd.DataFrame(meteorite_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling NA years :\n",
    "df[\"year\"] = df[\"year\"].astype(\"Int64\")\n",
    "df[\"year\"] = df[\"year\"].fillna(pd.NA)\n",
    "# handling NA types\n",
    "df[\"type\"] = df[\"type\"].replace(\"Unknown\", pd.NA)\n",
    "df[\"type\"] = df[\"type\"].replace(\"Unknown\", pd.NA)\n",
    "# handling NA countries\n",
    "df[\"country\"] = df[\"country\"].replace(\"Unknown\", pd.NA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Okay we have a dataset, let's try some simple commands :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df[df[\"type\"] == \"Iron, IIE-an\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df[df[\"name\"].str.lower().str.contains(\"catalina\", na=False)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df[df[\"type\"].str.lower().str.contains(\"l3\", na=False)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df[df[\"country\"].str.lower().str.contains(\"france\", na=False)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.dtypes)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
